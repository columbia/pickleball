# Evaluation

PickleBall's evaluation (Section 6) addresses four Research Questions and makes
the following claims:

* RQ1: PickleBall generates policies that blocks all malicious models from
  loading.
* RQ2: PickleBall generates policies that loads 79% of benign models.
* RQ3: PickleBall generates policies in a reasonable amount of time, and it
  enforces policies with minimal overhead.
* RQ4: PickleBall compares favorably to other state of the art tools.

This README provides steps to reproduce these claims. These steps assume that
you have access to this code repository and the malicious and benign models
distributed in the PickleBall artifact.

The steps below demonstrate how to reproduce the data provided in Table 1,
Table 2, Figure 6, and Figure 7 of the PickleBall paper.

TODO: map the tables and figures to the RQs being addressed.

## Preparation

Download all models the evaluation dataset from the following locations:
- Benign models: TODO
- Malicious models: TODO

Decompress each at a location outside of this code repository.

1. Create a `.env` file in the root of this code repository that sets the
following environment variables:

```
BENIGN_MODELS=<path to benign models>
MALICIOUS_MODELS=<path to malicious models>
```

(This file must be in the directory above this `evaluation/` directory.)

2. Build all docker containers.

From the repository root:

```
$ docker compose build
```

This will take approximately TODO minutes.

## Reproduce Policy Generation Table (Table 1)

Table 1 shows that when PickleBall generates policies for each library in the
dataset, ...

1. Generate policies for all libraries in the PickleBall evaluation dataset.

Command (from repository root):

```
$ docker compose run generate-all
```

Expected result:
* `evaluation/policies/`

Explanation:

This command invokes the following actions inside the policy generation
container:
1. Executes `evaluation/setup/fetch.sh` to fetch all evaluation libraries
  and apply our manual modifications, which can be inspected in
  `evaluation/setup/*.diff` files.
2. Executes `evaluation/generate-policies.py` to analyzes and generates policies
  for each library. This script is configured based on the
  `evaluation/manifest.toml` file, which specifies how the `pickleball-generate`
  program is invoked for each library in the evaluation dataset.
  **NOTE:** non-determinism ...

Note:

Output file:
* `evaluation/tables/table.pdf`: this table should reproduce the values in
  Table 1.

Configuration files:
* `evaluation/manifest.toml`: specifies how the `pickleball-generate` program
  is invoked for each library in the evaluation dataset.
* `evaluation/policies/baseline/<library>.json`: baseline policies for each
  library in evaluation dataset, generated by tracing the models in the benign
  model dataset. See [TODO](#TODO) section for recreating these, if desired.

Intermediate artifacts:
* `evaluation/policies/<library>.json`: the generated policy generated by
  `pickleball-generate` for a given library.

Results table can be produced with the `scripts/generatetable.py` script,
once all enforcement results have been produced (next step).


## RQ1: Malicious Model Blocking
TODO:


## RQ4: Comparison to SOTA

### ModelScan (20 mins)
Please make sure there is a `model-list.txt` file under the model path.
@andreas, please make sure the two model-list files are included in dataset, thanks!

```sh
docker compose run modelscan
```

The expected results:
```
Tool        #TP     #TN     #FP     #FN     FPR     FNR
ModelScan   75      236     16      9       6.3%    10.7%
```

### ModelTracer (75 mins)
```sh
sh RQ4/eval-modeltracer.sh
```

The expected results:
```
Tool        #TP     #TN     #FP     #FN     FPR     FNR
ModelTracer 43      252     0       41      0%      48.8%
```

**Note**: The above shows 43 TP detections while the paper reports 44. One model hangs in interactive mode and should be manually verified. The steps are in follows:
```sh
docker run -dit --name modeltracer_container modeltracer:latest
docker cp $MALICIOUS_MODEL/mkiani/gpt2-exec/gpt2-exec/pytorch_model.bin modeltracer_container:/root/modeltracer/pytorch_model.bin
docker exec -it modeltracer_container /bin/sh
python3 -m scripts.model_tracer /root/modeltracer/pytorch_model.bin torch
python3 -m scripts.parse_tracer
```


### Weights-Only (5 mins)
```sh
docker compose run weightsonly
```

The expected results:
```
Tool        #TP     #TN     #FP     #FN     FPR     FNR
WeightsOnly 84      157     95      0       37.7%   0.0%
```

### PickleBall
The results are already obtained and explained in RQ1 and RQ2.



## Steps to add library to evaluation

1. Does the library APIs permit arbitrary pickle behaviors?
2. Does the library distribute models on Hugging Face (or other platform?)
3. Can you load the available models using the library APIs?
4. What callables are in the available models?
5. What callables does PickleBall identify in the model loading policy?
6. Can PickleBall load the available models while enforcing the model loading
   policy?
7. Does PickleBall reject all malicious models when enforcing the model loading
   policy?

1. Ensure that library can load benign example model and is vulnerable to a
   backdoor model.

    a. Download example benign model.
    b. Create a backdoor version of the example model.
    c. Create a loading program that interfaces with the enforce/load_all.py
       script.
    d. Add library to manifest and fetch.sh
    e. Run the loading program to ensure that the benign model is loaded
       correctly, and the backdoor model executes its malicious payload. (add
       to docker compose task)

2. Evaluate PickleBall on the library and model

    f. Create an initial baseline trace from the benign model
        - fickling --trace
        - parsetrace
        - modelunion
    g. Use PickleBall to generate a policy for the library (add to manifest)
        - first do dry run to find Joern name for model class (manual)
    h. Create enforce container for the model
    i. Enforce model loading
        - create service in docker-compose.yml

3. Identify more benign models and reproduce result with full baseline trace

4. Evaluate malicious models on policy
