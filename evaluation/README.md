# Evaluation

* RQ1: TODO
* RQ2: TODO
* RQ3: TODO
* RQ4: TODO

Additional evaluation claims: TODO

## Preparation

Download models (benign and malicious) (TODO: provide) at some path outside of this git 
repository directory.

TODO: prepare datasets directory.

Create a .env file in this git repository root that sets the following environment
variables:

```
BENIGN_MODELS=<path to benign models>
MALICIOUS_MODELS=<path to malicious models>
DATASETS=<path to datasets>
```





## Reproduce Policy Generation Table (Table 1)

From root project directory:

```
docker compose run generate-all
```

This invokes the following actions:
1. Builds all necessary docker containers (specified in `docker-compose.yml`)
2. Fetches all evaluation libraries (`evaluation/setup/fetch.sh`)
3. Analyzes and generates policies for each library (`evaluation/generate-policies.py`)


Output file:
* `evaluation/tables/table.pdf`: this table should reproduce the values in
  Table 1.

Configuration files:
* `evaluation/manifest.toml`: specifies how the `pickleball-generate` program
  is invoked for each library in the evaluation dataset.
* `evaluation/policies/baseline/<library>.json`: baseline policies for each
  library in evaluation dataset, generated by tracing the models in the benign
  model dataset. See [TODO](#TODO) section for recreating these, if desired.

Intermediate artifacts:
* `evaluation/policies/<library>.json`: the generated policy generated by
  `pickleball-generate` for a given library.

Results table can be produced with the `scripts/generatetable.py` script,
once all enforcement results have been produced (next step).


## RQ1: Malicious Model Blocking
TODO:


## RQ4: Comparison to SOTA
### ModelScan (20 mins)
Please make sure there is a `model-list.txt` file under the model path.
@andreas, please make sure the two model-list files are included in dataset, thanks!

```sh
docker compose run modelscan
```

The expected results:
```
Tool        #TP     #TN     #FP     #FN     FPR     FNR
ModelScan   75      236     16      9       6.3%    10.7%
```

### ModelTracer (75 mins)
```sh
sh RQ4/eval-modeltracer.sh
```

The expected results:
```
Tool        #TP     #TN     #FP     #FN     FPR     FNR
ModelTracer 43      252     0       41      0%      48.8%
```

**Note**: The above shows 43 TP detections while the paper reports 44. One model hangs in interactive mode and should be manually verified. The steps are in follows:
```sh
docker run -dit --name modeltracer_container modeltracer:latest
docker cp $MALICIOUS_MODEL/mkiani/gpt2-exec/gpt2-exec/pytorch_model.bin modeltracer_container:/root/modeltracer/pytorch_model.bin
docker exec -it modeltracer_container /bin/sh
python3 -m scripts.model_tracer /root/modeltracer/pytorch_model.bin torch
python3 -m scripts.parse_tracer
```


### Weights-Only (5 mins)
```sh
docker compose run weightsonly
```

The expected results:
```
Tool        #TP     #TN     #FP     #FN     FPR     FNR
WeightsOnly 84      157     95      0       37.7%   0.0%
```

### PickleBall
The results are already obtained and explained in RQ1 and RQ2.



## Steps to add library to evaluation

1. Does the library APIs permit arbitrary pickle behaviors?
2. Does the library distribute models on Hugging Face (or other platform?)
3. Can you load the available models using the library APIs?
4. What callables are in the available models?
5. What callables does PickleBall identify in the model loading policy?
6. Can PickleBall load the available models while enforcing the model loading
   policy?
7. Does PickleBall reject all malicious models when enforcing the model loading
   policy?

1. Ensure that library can load benign example model and is vulnerable to a
   backdoor model.

    a. Download example benign model.
    b. Create a backdoor version of the example model.
    c. Create a loading program that interfaces with the enforce/load_all.py
       script.
    d. Add library to manifest and fetch.sh
    e. Run the loading program to ensure that the benign model is loaded
       correctly, and the backdoor model executes its malicious payload. (add
       to docker compose task)

2. Evaluate PickleBall on the library and model

    f. Create an initial baseline trace from the benign model
        - fickling --trace
        - parsetrace
        - modelunion
    g. Use PickleBall to generate a policy for the library (add to manifest)
        - first do dry run to find Joern name for model class (manual)
    h. Create enforce container for the model
    i. Enforce model loading
        - create service in docker-compose.yml

3. Identify more benign models and reproduce result with full baseline trace

4. Evaluate malicious models on policy
