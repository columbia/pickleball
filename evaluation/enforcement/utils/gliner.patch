diff --git a/gliner/model.py b/gliner/model.py
index 5d8d730..7a0d9af 100644
--- a/gliner/model.py
+++ b/gliner/model.py
@@ -88,7 +88,7 @@ class GLiNER(nn.Module, PyTorchModelHubMixin):
         if config.vocab_size != -1 and config.vocab_size != len(
             self.data_processor.transformer_tokenizer
         ):
-            warnings.warn(f"""Vocab size of the model ({config.vocab_size}) does't match length of tokenizer ({len(self.data_processor.transformer_tokenizer)}). 
+            warnings.warn(f"""Vocab size of the model ({config.vocab_size}) does't match length of tokenizer ({len(self.data_processor.transformer_tokenizer)}).
                             You should to consider manually add new tokens to tokenizer or to load tokenizer with added tokens.""")
 
         if isinstance(self.model, BaseORTModel):
@@ -809,7 +809,7 @@ class GLiNER(nn.Module, PyTorchModelHubMixin):
                     for key in f.keys():
                         state_dict[key] = f.get_tensor(key)
             else:
-                state_dict = torch.load(model_file, map_location=torch.device(map_location), weights_only=True)
+                state_dict = torch.load(model_file, map_location=torch.device(map_location), weights_only=False)
             gliner.model.load_state_dict(state_dict, strict=strict)
             gliner.model.to(map_location)
             if compile_torch_model and "cuda" in map_location:
