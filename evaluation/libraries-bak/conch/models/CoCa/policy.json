{"open_clip_custom/coca_model.py:<module>.CoCa":{"globals":["torch.float8_e4m3fnuz","open_clip_custom.transformer.TextTransformer.set_grad_checkpointing","open_clip_custom.coca_model.CoCa.lock_image_tower","typing.Optional.<indexAccess>.masked_fill","torch._utils._rebuild_nested_tensor","torch.complex32","torch.BoolStorage","open_clip_custom.transformer.MultimodalTransformer.forward","torch.DoubleStorage","open_clip_custom.transformer.TextTransformer.init_parameters","torch.uint16","torch.ShortStorage","open_clip_custom.coca_model._build_text_decoder_tower","torch.per_channel_affine_float_qparams","torch._utils._rebuild_parameter_with_state","open_clip_custom.transformer.Transformer.__init__","open_clip_custom.vision_tower.VisualModel.forward_project","torch.bits2x4","torch.uint64","torch._utils._rebuild_tensor_v2","open_clip_custom.transformer.AttentionalPooler.__init__","open_clip_custom.transformer.TextTransformer.__init__","torch.Size","builtins.bytearray","torch.ComplexFloatStorage","torch._utils._rebuild_sparse_tensor","torch.HalfStorage","open_clip_custom.transformer.Transformer.forward","torch.nn.parameter.Parameter","open_clip_custom.coca_model.CoCa.encode_image","open_clip_custom.vision_tower.VisualModel.forward_attn_pool_caption","torch._utils._rebuild_wrapper_subclass","torch.Tensor","torch.LongStorage","_codecs.encode","open_clip_custom.vision_tower.VisualModel.__init__","open_clip_custom.vision_tower.VisualModel.set_grad_checkpointing","open_clip_custom.coca_model.CoCa.lock_temperature","torch.device","torch.TypedStorage","torch.QUInt2x4Storage","open_clip_custom.vision_tower.VisualModel._global_pool","timm.models.layers.to_2tuple","torch._utils._rebuild_device_tensor_from_numpy","open_clip_custom.transformer.MultimodalTransformer.build_attention_mask","open_clip_custom.transformer.TextTransformer.build_attention_mask","open_clip_custom.transformer.TextTransformer","open_clip_custom.transformer.Transformer.get_cast_dtype","torch.zeros_like.<returnValue>.<indexAccess>.view","open_clip_custom.coca_model.CoCa.encode_text","torch.per_channel_symmetric","torch.float8_e4m3fn","torch.CharStorage","torch.float8_e5m2fnuz","open_clip_custom.transformer.MultimodalTransformer.lock_self_attention","torch.per_channel_affine","torch.bits8","torch.QInt32Storage","open_clip_custom.coca_model.CoCa.__init__","open_clip_custom.transformer.MultimodalTransformer.set_grad_checkpointing","collections.Counter","open_clip_custom.transformer.MultimodalTransformer.set_mask_prob","torch._utils._rebuild_parameter","open_clip_custom.coca_model.CoCa.forward","torch.ByteStorage","collections.OrderedDict","torch._tensor._rebuild_from_type_v2","torch.FloatStorage","torch._utils._rebuild_tensor","open_clip_custom.transformer.MultimodalTransformer.init_parameters","typing.Optional.masked_fill","torch.nn.serialization._get_layout","open_clip_custom.coca_model.CoCa.set_grad_checkpointing","open_clip_custom.coca_model.CoCa.generate","torch._utils._rebuild_tensor_v3","torch.bits4x2","open_clip_custom.vision_tower.VisualModel","open_clip_custom.transformer.TextTransformer._repeat","torch.uint32","open_clip_custom.vision_tower.VisualModel.forward_no_head","open_clip_custom.transformer.MultimodalTransformer","torch._utils._rebuild_device_tensor_from_cpu_tensor","open_clip_custom.transformer.TextTransformer.forward","torch.float8_e5m2","torch.IntStorage","torch.QUInt4x2Storage","torch.bits1x8","open_clip_custom.transformer.TextTransformer.build_cls_mask","torch.per_tensor_symmetric","open_clip_custom.transformer.Transformer","torch._utils._rebuild_qtensor","torch.UntypedStorage","typing.Optional.view","open_clip_custom.vision_tower.VisualModel.forward","torch.zeros_like.<returnValue>.masked_fill","open_clip_custom.transformer.MultimodalTransformer.__init__","open_clip_custom.coca_model._build_vision_tower","open_clip_custom.vision_tower.VisualModel.lock","open_clip_custom.coca_model.CoCa","open_clip_custom.coca_model.CoCa._encode_text","typing.Optional","torch.BFloat16Storage","torch.QInt8Storage","torch.QUInt8Storage","torch.per_tensor_affine","torch.zeros_like.<returnValue>.<indexAccess>.masked_fill","open_clip_custom.coca_model.CoCa._encode_image","torch.bits16","torch.ComplexDoubleStorage","open_clip_custom.coca_model._build_text_tower","torch._utils._rebuild_meta_tensor_no_storage"],"reduces":["torch._utils._rebuild_tensor_v2","torch.Size","torch.nn.parameter.Parameter","torch._utils._rebuild_wrapper_subclass","torch.device","torch._utils._rebuild_device_tensor_from_numpy","torch.float8_e4m3fnuz","torch.per_tensor_symmetric","torch.BFloat16Storage","torch._utils._rebuild_meta_tensor_no_storage","torch._utils._rebuild_nested_tensor","torch.complex32","torch.BoolStorage","torch.DoubleStorage","torch.uint16","torch.ShortStorage","torch._utils._rebuild_parameter_with_state","torch.per_channel_affine_float_qparams","torch.bits2x4","torch.uint64","builtins.bytearray","torch.ComplexFloatStorage","torch._utils._rebuild_sparse_tensor","torch.HalfStorage","torch.Tensor","torch.LongStorage","_codecs.encode","torch.TypedStorage","torch.QUInt2x4Storage","torch.per_channel_symmetric","torch.float8_e4m3fn","torch.CharStorage","torch.float8_e5m2fnuz","torch.per_channel_affine","torch.bits8","torch.QInt32Storage","collections.Counter","torch._utils._rebuild_parameter","torch.ByteStorage","collections.OrderedDict","torch._tensor._rebuild_from_type_v2","torch.FloatStorage","torch._utils._rebuild_tensor","torch.nn.serialization._get_layout","torch._utils._rebuild_tensor_v3","torch.bits4x2","torch.uint32","torch._utils._rebuild_device_tensor_from_cpu_tensor","torch.float8_e5m2","torch.IntStorage","torch.QUInt4x2Storage","torch.bits1x8","torch._utils._rebuild_qtensor","torch.UntypedStorage","torch.QInt8Storage","torch.QUInt8Storage","torch.per_tensor_affine","torch.bits16","torch.ComplexDoubleStorage"]}}