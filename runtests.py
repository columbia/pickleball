"""
Run static inference on test suite defined in FIXTURES.

For each sub-directory in the pickle-defense/analyze/tests/ directory:
- Assume subdir has src/ directory containing test target.
- Assume subdir has models/ directory containing class subdirectories
  - Each class subdir contains a 'metadata' file containing the Joern
     fullname for the class type decl.
  - Each class subdir contains a baseline.json file with a 'perfect' baseline
      reference. This can be generated by using the modelunion script from a
      set of valid model traces.

TODO: produce a test report
"""

# - Generate fickling trace and json policy description for each model
#   - input: subdir/class/*.pkl
#   - output: subdir/class/*.trace
#   - output: subdir/class/*.json
# - Generate the fixture baseline policy for each class
#   (union of all policies generated per model)
#   - input: subdir/class/*.json
#   - input: subdir/class/metadata
#   - output: subdir/class/baseline.json
# - Generate Joern CPG
#   - input: subdir/src/
#   - output: subdir/out.cpg
# - Generate inferred policy for each class
#   - input: subdir/out.cpg
#   - input: subdir/*/metadata
#   - output: subdir/*/inferred.json
# - Report F1 score for each inferred policy compared to baseline:
#   - input: subdir/*/inferred.json
#   - input: subdir/*/baseline.json
#   - output: subdir/*/result.json

import argparse
import pathlib
import subprocess
from typing import Tuple

from scripts import compare

FIXTURES = [
    'no-inheritance',
    'simple-inheritance',
    'simplest-inheritance',
    'multiple-inheritance',
    'nested-inheritance',
    'dictionary-types',
    'interprocedural-attribute-writes',
    'reduce',
    #'follow-collection-object'
]

# Hardcoded for docker container paths
# TODO: Make configurable
PATH_TO_JOERN = pathlib.Path('/joern')
PATH_TO_ANALYSIS_SCRIPT = pathlib.Path('/pickle-defense/analyze/analyze.sc')
PATH_TO_FIXTURES = pathlib.Path('/pickle-defense/analyze/tests/')

RED = '\033[91m'
GREEN = '\033[92m'
BLUE = '\033[94m'
RESET = '\033[0m'

def create_cpg(fixture_path: pathlib.Path, output_path: pathlib.Path) -> bool:
    """Create Joern CPG for test fixture by invoking joern-parse

    Writes CPG in test fixture directory.
    """

    print(f'- Creating CPG: {fixture_path}')
    joern_parse_path = PATH_TO_JOERN / pathlib.Path('joern-parse')
    target_code = fixture_path / pathlib.Path('src')
    command = [str(joern_parse_path), str(target_code), '-o', str(output_path)]
    print(f'- Command: {" ".join(command)}')
    result = subprocess.run(command, capture_output=True, check=False)

    return result.returncode == 0


# Create inferred policy by running analysis script
def infer_policy(cpg_path: pathlib.Path, model_class: str, output_path: pathlib.Path) -> str:
    """Run inference analysis for a given CPG and model class by invoking joern.

    Writes the analysis output as JSON to the output_path.
    """

    print('- Inferring policy')
    joern_path = PATH_TO_JOERN / pathlib.Path('joern')
    command = [
        str(joern_path),
        '--script',
        str(PATH_TO_ANALYSIS_SCRIPT),
        '--param',
        f'inputPath={str(cpg_path)}',
        '--param',
        f'modelClass={model_class}',
        '--param',
        f'outputPath={str(output_path)}'
    ]
    print(f'- Command: {" ".join(command)}')
    result = subprocess.run(command, capture_output=True, check=False)

    if result.returncode != 0:
        print(result.stderr)

    return result.returncode == 0

def compare_policies(policy: pathlib.Path, baseline: pathlib.Path) -> Tuple[float, float]:

    result = compare.compare_json_files(str(policy), str(baseline))
    global_scores = result["global_lines"]
    reduce_scores = result["reduce_lines"]
    return (global_scores, reduce_scores)


def print_fixtures():
    list(map(print, FIXTURES))

def main():

    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--list',
        action='store_true',
        help='List all available test fixtures'
    )
    parser.add_argument(
        '--fixtures',
        nargs='*',
        help='A list of individual tests to execute. If none provided, defaults to running all fixtures',
        default=[])
    args = parser.parse_args()

    if args.list:
        print_fixtures()
        return

    if not args.fixtures:
        fixtures = FIXTURES
    else:
        assert all(fixture in FIXTURES for fixture in args.fixtures), "Invalid fixture provided"
        fixtures = args.fixtures

    test_results = {}
    for fixture in fixtures:
        print(f'{BLUE}[*] Test fixture: {fixture}{RESET}')
        fixture_path = PATH_TO_FIXTURES / pathlib.Path(fixture)
        cpg_path = fixture_path / pathlib.Path('out.cpg')

        if not create_cpg(fixture_path, cpg_path):
            print(f'error creating CPG for fixture {fixture}')
            return -1

        # For each model class, infer a policy and compare it to the policy
        # baseline.
        models_path = fixture_path / pathlib.Path('models')
        model_dirs = [subdir for subdir in models_path.iterdir() if subdir.is_dir()]
        for model_dir in model_dirs:
            model_class_file = model_dir / pathlib.Path('metadata')
            with open(str(model_class_file), 'r', encoding='utf-8') as metadata_fd:
                model_class_name = metadata_fd.read().strip()
            inferred_path = model_dir / pathlib.Path('inferred.json')
            if not infer_policy(cpg_path, model_class_name, inferred_path):
                print(f'error inferring policy for model {model_dir.name}')
                return -1

            baseline_path = model_dir / pathlib.Path('baseline.json')
            global_scores, reduce_scores = compare_policies(inferred_path, baseline_path)

            if global_scores["precision"] < 1.0 or reduce_scores["precision"] < 1.0:
                print(f"{RED}[-] FAIL{RESET}")
                test_results[f'{fixture}-{model_class_name}'] = 'FAIL'
            else:
                print(f"{GREEN}[+] PASS{RESET}")
                test_results[f'{fixture}-{model_class_name}'] = 'PASS'

            print(f"- globals F1: {global_scores['f1']}")
            print(f"- globals precision: {global_scores['precision']}")
            print(f"- globals recall: {global_scores['recall']}")
            print(f"- reduces F1: {reduce_scores['f1']}")
            print(f"- reduces precision: {reduce_scores['precision']}")
            print(f"- reduces recall: {reduce_scores['recall']}")

    print(f'Tests passed: {sum(value == "PASS" for value in test_results.values())} / {len(test_results)}')
if __name__ == '__main__':
    main()
